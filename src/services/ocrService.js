// ocrService.js - Enhanced with preprocessing and AI extraction
import vision from '@google-cloud/vision'
import dotenv from 'dotenv'
import { preprocessOCRText } from './textPreprocessingService.js'
import { extractFieldsWithAI, validateRequiredFields } from './aiFieldExtractionService.js'

// Load environment variables (same as supabase.js does)
dotenv.config()

// Create client with credentials from environment variable
let client
try {  
  if (process.env.GOOGLE_CLOUD_CREDENTIALS) {
    // Use JSON credentials from environment variable (for production)
    const credentials = JSON.parse(process.env.GOOGLE_CLOUD_CREDENTIALS)
    client = new vision.ImageAnnotatorClient({ credentials })
  } else if (process.env.GOOGLE_APPLICATION_CREDENTIALS && !process.env.GOOGLE_APPLICATION_CREDENTIALS.startsWith('{')) {
    // Use file path (for development) - only if it doesn't start with '{'
    client = new vision.ImageAnnotatorClient()
  } else {
    // Fallback to default authentication
    client = new vision.ImageAnnotatorClient()
  }
} catch (error) {
  console.error('âŒ Error initializing Google Vision client:', error)
  // Fallback to default authentication
  console.log('ðŸ”„ Falling back to default authentication')
  client = new vision.ImageAnnotatorClient()
}

// Helper function: parse text and extract fields with confidence
function parseOCRTextWithConfidence(text, ocrResponse) {
  const lines = text.split('\n').map(l => l.trim()).filter(Boolean)
  const fields = {}
  const confidences = {}

  // Flatten all symbols for approximate confidence per line
  const symbols = ocrResponse?.fullTextAnnotation?.pages.flatMap(p =>
    p.blocks.flatMap(b =>
      b.paragraphs.flatMap(par =>
        par.words.flatMap(w =>
          w.symbols.map(s => s)
        )
      )
    )
  ) || []

  function getLineConfidence(line) {
    const matchingSymbols = symbols.filter(s => line.includes(s.text))
    if (matchingSymbols.length === 0) return 0
    return matchingSymbols.reduce((sum, s) => sum + (s.confidence || 0), 0) / matchingSymbols.length
  }

  for (let i = 0; i < lines.length; i++) {
    const line = lines[i]
    const nextLine = lines[i + 1] || ''
    const lineConfidence = getLineConfidence(line)
    

    // Report Number
    if (/×ž×¡×¤×¨ ×“×•×—|×¤×¨×˜×™ ×“×•×— ×ž×¡×¤×¨/i.test(line)) {
      const match = line.match(/\d{6,}/) || nextLine.match(/\d{6,}/)
      if (match) {
        fields.reportNumber = match[0]
        confidences.reportNumber = lineConfidence
      }
    }

    // Violation Date and Time
    if (/×ª××¨×™×š ×¢×‘×™×¨×”/i.test(line)) {
      const match = line.match(/(\d{2}\/\d{2}\/\d{4})/)
      const timeMatch = line.match(/(\d{2}:\d{2})/)
      if (match) {
        fields.violationDate = match[1]
        confidences.violationDate = lineConfidence
      }
      if (timeMatch) {
        fields.violationTime = timeMatch[1]
        confidences.violationTime = lineConfidence
      }
    }

    // Location
    if (/×¨×—×•×‘/i.test(line) || /×ž×™×§×•×/i.test(line)) {
      const loc = line.replace(/×¨×—×•×‘:|×ž×™×§×•× ×™×—×¡×™:/i, '').trim() + (nextLine && !nextLine.match(/\d{2}\/\d{2}\/\d{4}/) ? ' ' + nextLine : '')
      fields.location = loc.trim()
      confidences.location = lineConfidence
    }

    // Violation Type / Legal Section
    if (/×¡×¢×™×£ ×”×¢×‘×™×¨×”/i.test(line) || /×ž×¡' ×¢×‘×™×¨×”/i.test(line)) {
      const match = line.match(/(\d+\s*\(?.*?\)?)/)
      if (match) {
        fields.violationType = match[1].trim()
        confidences.violationType = lineConfidence
      }
    }

    // Fine Amount
    if (/×¡×›×•× ×œ×ª×©×œ×•×/i.test(line)) {
      const match = line.match(/(\d+)/)
      if (match) {
        fields.fineAmount = match[1]
        confidences.fineAmount = lineConfidence
      }
    }

    // Driver Name
    if (/××œ ×”× ×”×’/i.test(line)) {
      fields.driverName = line.replace('××œ ×”× ×”×’', '').trim()
      confidences.driverName = lineConfidence
    }

    // License Number
    if (/×ž×¡×¤×¨ ×¨×™×©×•×™/i.test(line)) {
      fields.licenseNumber = nextLine || ''
      confidences.licenseNumber = lineConfidence
    }

    // Vehicle Plate (if present)
    if (/×œ×•×—×™×ª ×™×©×¨××œ×™|×ž×¡×¤×¨ ×¨×›×‘/i.test(line)) {
      const match = nextLine.match(/(\d{8})/) || line.match(/(\d{8})/)
      if (match) {
        fields.vehiclePlate = match[1]
        confidences.vehiclePlate = lineConfidence
      }
    }

    // Points (if present)
    if (/× ×§×•×“×•×ª/i.test(line)) {
      const match = line.match(/(\d+)/)
      if (match) {
        fields.points = match[1]
        confidences.points = lineConfidence
      }
    }

    // Appeal Deadline (if present)
    if (/×ž×•×¢×“ ××—×¨×•×Ÿ ×œ×¢×¨×¢×•×¨|×ž×•×¢×“ ×¢×¨×¢×•×¨|×ª××¨×™×š ×¢×¨×¢×•×¨|×¢×¨×¢×•×¨ ×¢×“|×ž×•×¢×“ ××—×¨×•×Ÿ ×œ×ª×©×œ×•×/i.test(line)) {
      // Check current line and up to 5 lines ahead for date pattern
      const line2 = lines[i + 2] || ''
      const line3 = lines[i + 3] || ''
      const line4 = lines[i + 4] || ''
      const line5 = lines[i + 5] || ''
      const match = line.match(/(\d{2}\/\d{2}\/\d{4})/) || 
                   nextLine.match(/(\d{2}\/\d{2}\/\d{4})/) ||
                   line2.match(/(\d{2}\/\d{2}\/\d{4})/) ||
                   line3.match(/(\d{2}\/\d{2}\/\d{4})/) ||
                   line4.match(/(\d{2}\/\d{2}\/\d{4})/) ||
                   line5.match(/(\d{2}\/\d{2}\/\d{4})/)
      if (match) {
        fields.appealDeadline = match[1]
        confidences.appealDeadline = lineConfidence
      } else {
      }
    }
  }

  return { extractedFields: fields, confidenceScores: confidences }
}


// Main function - Enhanced with preprocessing and AI extraction
export async function extractTextFromDocument(fileInfo) {
  const { buffer, originalName, mimetype } = fileInfo
  const startTime = Date.now()

  try {
    // Step 1: Google Vision OCR
    const [result] = await client.documentTextDetection({ image: { content: buffer } })
    const rawText = result.fullTextAnnotation?.text || ''
    
    if (!rawText.trim()) {
      throw new Error('No text detected in document')
    }


    // Calculate overall OCR confidence from Vision API
    const ocrConfidence = calculateVisionConfidence(result)

    // Step 2: Text preprocessing and normalization
    const preprocessedText = preprocessOCRText(rawText)

    // Step 3: AI-based field extraction
    const aiExtractionResult = await extractFieldsWithAI(preprocessedText, ocrConfidence)

    if (!aiExtractionResult.success) {
      console.warn('âš ï¸ AI extraction failed, falling back to legacy parsing')
      // Fallback to legacy parsing
      const { extractedFields, confidenceScores } = parseOCRTextWithConfidence(rawText, result)
      console.log('ðŸ” OCR Debug - Legacy extraction fields:', extractedFields)
      console.log('ðŸ” OCR Debug - Legacy confidence scores:', confidenceScores)
      return createLegacyResult(rawText, extractedFields, confidenceScores, fileInfo)
    }

    // Step 4: Validate required fields
    const validation = validateRequiredFields(aiExtractionResult, 0.6)

    const processingTime = Date.now() - startTime
    console.log(`âœ… Enhanced OCR complete: ${processingTime}ms, confidence: ${(ocrConfidence * 100).toFixed(1)}%, fields: ${Object.keys(aiExtractionResult.extractedFields).length}`)

    // Check if AI missed any critical fields that legacy might catch
    const missingFields = ['points', 'appealDeadline', 'vehiclePlate'].filter(field => 
      !aiExtractionResult.extractedFields[field] || aiExtractionResult.extractedFields[field] === null
    )
    
    let finalFields = aiExtractionResult.extractedFields
    let finalConfidences = aiExtractionResult.confidenceScores
    
    // Only run legacy extraction if AI missed important fields
    if (missingFields.length > 0) {
      const legacyResult = parseOCRTextWithConfidence(rawText, result)
      
      // Fill in only the missing fields from legacy extraction
      missingFields.forEach(field => {
        if (legacyResult.extractedFields[field]) {
          finalFields = { ...finalFields, [field]: legacyResult.extractedFields[field] }
          finalConfidences = { ...finalConfidences, [field]: legacyResult.confidenceScores[field] || 0 }
        }
      })
    }

    // Return enhanced OCR results
    
    return {
      // Original data
      extractedText: rawText,
      
      // Enhanced extraction results
      extractedFields: finalFields,
      confidenceScores: finalConfidences,
      
      // Processing pipeline results
      preprocessing: {
        normalizedText: preprocessedText.normalizedText,
        correctedText: preprocessedText.correctedText,
        detectedFields: preprocessedText.detectedFields,
        processingInfo: preprocessedText.processingInfo
      },
      
      // AI extraction details
      aiExtraction: {
        processingNotes: aiExtractionResult.processingNotes,
        aiUsage: aiExtractionResult.aiUsage
      },
      
      // Validation results
      validation: validation,
      
      // Processing metadata
      processingInfo: {
        fileType: mimetype,
        fileSize: buffer.length,
        processedAt: new Date().toISOString(),
        ocrEngine: 'Google Vision + AI Enhancement',
        processingTime: `${processingTime}ms`,
        ocrConfidence: ocrConfidence,
        pipeline: 'enhanced',
        isPDF: mimetype === 'application/pdf'
      }
    }

  } catch (error) {
    console.error('âŒ Enhanced OCR processing failed:', error)
    
    // Fallback to legacy processing
    console.log('ðŸ”„ Falling back to legacy OCR processing...')
    try {
      const [result] = await client.documentTextDetection({ image: { content: buffer } })
      const extractedText = result.fullTextAnnotation?.text || ''
      const { extractedFields, confidenceScores } = parseOCRTextWithConfidence(extractedText, result)
      
      return createLegacyResult(extractedText, extractedFields, confidenceScores, fileInfo, error.message)
    } catch (fallbackError) {
      throw new Error(`Both enhanced and legacy OCR failed: ${error.message}, ${fallbackError.message}`)
    }
  }
}



// Helper function to validate extracted fields
export const validateExtractedFields = (fields) => {
  const required = ['reportNumber', 'violationDate', 'violationType', 'fineAmount']
  const missing = required.filter(field => !fields[field])

  return {
    isValid: missing.length === 0,
    missingFields: missing,
    completeness: ((required.length - missing.length) / required.length) * 100
  }
}

// Helper function to calculate overall confidence from Google Vision API
function calculateVisionConfidence(visionResult) {
  try {
    const pages = visionResult.fullTextAnnotation?.pages || []
    if (pages.length === 0) return 0
    
    let totalConfidence = 0
    let symbolCount = 0
    
    for (const page of pages) {
      for (const block of page.blocks || []) {
        for (const paragraph of block.paragraphs || []) {
          for (const word of paragraph.words || []) {
            for (const symbol of word.symbols || []) {
              if (symbol.confidence !== undefined) {
                totalConfidence += symbol.confidence
                symbolCount++
              }
            }
          }
        }
      }
    }
    
    return symbolCount > 0 ? totalConfidence / symbolCount : 0.5
  } catch (error) {
    console.warn('Failed to calculate Vision confidence:', error)
    return 0.5 // Default confidence
  }
}

// Helper function to create legacy result format
function createLegacyResult(extractedText, extractedFields, confidenceScores, fileInfo, errorMessage = null) {
  return {
    extractedText,
    extractedFields,
    confidenceScores,
    processingInfo: {
      fileType: fileInfo.mimetype,
      fileSize: fileInfo.buffer.length,
      processedAt: new Date().toISOString(),
      ocrEngine: 'Google Vision (Legacy)',
      pipeline: 'legacy',
      isPDF: fileInfo.mimetype === 'application/pdf',
      fallbackReason: errorMessage
    },
    validation: {
      isValid: Object.keys(extractedFields).length > 0,
      missingFields: [],
      lowConfidenceFields: [],
      completeness: 50 // Estimate for legacy
    }
  }
}

// Helper function to improve field extraction confidence
export const enhanceFieldExtraction = (extractedText) => {
  // This would contain regex patterns and NLP logic to improve field extraction
  // For now, return the mock data
  return {
    enhanced: true,
    improvements: ['Date format standardized', 'Currency symbols normalized']
  }
}
